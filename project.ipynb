{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab516c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "try:\n",
    "    fastf1.Cache.enable_cache(\"f1_cache\")\n",
    "except Exception as e:\n",
    "    print(f\"Error enabling cache: {e}. Check cache directory permissions.\")\n",
    "\n",
    "def get_driver_features(driver_code):\n",
    "\n",
    "    driver_characteristics = {\n",
    "     \n",
    "        'VER': {'WetWeatherSkill': 0.95, 'QualifyingPace': 0.95, 'RaceCraft': 0.95, 'Consistency': 0.90, 'Aggression': 0.90, 'TireManagement': 0.85},\n",
    "        'NOR': {'WetWeatherSkill': 0.85, 'QualifyingPace': 0.92, 'RaceCraft': 0.88, 'Consistency': 0.88, 'Aggression': 0.75, 'TireManagement': 0.87},\n",
    "        'PIA': {'WetWeatherSkill': 0.82, 'QualifyingPace': 0.88, 'RaceCraft': 0.84, 'Consistency': 0.85, 'Aggression': 0.78, 'TireManagement': 0.85},\n",
    "        'LEC': {'WetWeatherSkill': 0.82, 'QualifyingPace': 0.95, 'RaceCraft': 0.85, 'Consistency': 0.80, 'Aggression': 0.85, 'TireManagement': 0.80},\n",
    "        'HAM': {'WetWeatherSkill': 0.95, 'QualifyingPace': 0.90, 'RaceCraft': 0.95, 'Consistency': 0.92, 'Aggression': 0.80, 'TireManagement': 0.90},\n",
    "        'RUS': {'WetWeatherSkill': 0.80, 'QualifyingPace': 0.90, 'RaceCraft': 0.85, 'Consistency': 0.85, 'Aggression': 0.83, 'TireManagement': 0.82},\n",
    "        'ANT': {'WetWeatherSkill': 0.78, 'QualifyingPace': 0.85, 'RaceCraft': 0.78, 'Consistency': 0.76, 'Aggression': 0.85, 'TireManagement': 0.75},\n",
    "        'SAI': {'WetWeatherSkill': 0.84, 'QualifyingPace': 0.88, 'RaceCraft': 0.87, 'Consistency': 0.87, 'Aggression': 0.82, 'TireManagement': 0.85},\n",
    "        'ALO': {'WetWeatherSkill': 0.92, 'QualifyingPace': 0.90, 'RaceCraft': 0.95, 'Consistency': 0.90, 'Aggression': 0.88, 'TireManagement': 0.90},\n",
    "        'STR': {'WetWeatherSkill': 0.85, 'QualifyingPace': 0.75, 'RaceCraft': 0.80, 'Consistency': 0.75, 'Aggression': 0.75, 'TireManagement': 0.78},\n",
    "        'TSU': {'WetWeatherSkill': 0.78, 'QualifyingPace': 0.85, 'RaceCraft': 0.82, 'Consistency': 0.75, 'Aggression': 0.90, 'TireManagement': 0.78},\n",
    "        'HAD': {'WetWeatherSkill': 0.75, 'QualifyingPace': 0.80, 'RaceCraft': 0.78, 'Consistency': 0.74, 'Aggression': 0.85, 'TireManagement': 0.74},\n",
    "        'ALB': {'WetWeatherSkill': 0.80, 'QualifyingPace': 0.85, 'RaceCraft': 0.84, 'Consistency': 0.82, 'Aggression': 0.75, 'TireManagement': 0.83},\n",
    "        'GAS': {'WetWeatherSkill': 0.83, 'QualifyingPace': 0.84, 'RaceCraft': 0.85, 'Consistency': 0.83, 'Aggression': 0.80, 'TireManagement': 0.82},\n",
    "        'OCO': {'WetWeatherSkill': 0.80, 'QualifyingPace': 0.82, 'RaceCraft': 0.83, 'Consistency': 0.80, 'Aggression': 0.85, 'TireManagement': 0.78},\n",
    "        'HUL': {'WetWeatherSkill': 0.85, 'QualifyingPace': 0.82, 'RaceCraft': 0.80, 'Consistency': 0.83, 'Aggression': 0.75, 'TireManagement': 0.80},\n",
    "        'BOR': {'WetWeatherSkill': 0.74, 'QualifyingPace': 0.76, 'RaceCraft': 0.76, 'Consistency': 0.74, 'Aggression': 0.82, 'TireManagement': 0.75},\n",
    "        'DOO': {'WetWeatherSkill': 0.74, 'QualifyingPace': 0.78, 'RaceCraft': 0.75, 'Consistency': 0.73, 'Aggression': 0.85, 'TireManagement': 0.74},\n",
    "        'BEA': {'WetWeatherSkill': 0.75, 'QualifyingPace': 0.78, 'RaceCraft': 0.77, 'Consistency': 0.75, 'Aggression': 0.80, 'TireManagement': 0.76},\n",
    "        'LAW': {'WetWeatherSkill': 0.76, 'QualifyingPace': 0.79, 'RaceCraft': 0.78, 'Consistency': 0.75, 'Aggression': 0.85, 'TireManagement': 0.75},\n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    default_features = {\n",
    "        'WetWeatherSkill': 0.75, \n",
    "        'QualifyingPace': 0.75, \n",
    "        'RaceCraft': 0.75, \n",
    "        'Consistency': 0.75, \n",
    "        'Aggression': 0.75, \n",
    "        'TireManagement': 0.75\n",
    "    }\n",
    "    \n",
    "    # Return driver features if found, otherwise default\n",
    "    if driver_code in driver_characteristics:\n",
    "        return driver_characteristics[driver_code]\n",
    "    # Try to handle numeric driver codes from historical data\n",
    "    elif str(driver_code) in driver_characteristics:\n",
    "        return driver_characteristics[str(driver_code)]\n",
    "    else:\n",
    "        print(f\"Driver characteristics not found for '{driver_code}'. Using default values.\")\n",
    "        return default_features\n",
    "\n",
    "def get_standings_before_round(year, target_round):\n",
    "\n",
    "    driver_standings = {}\n",
    "    constructor_standings = {}\n",
    "    max_driver_points = 0\n",
    "    max_constructor_points = 0\n",
    "    try:\n",
    "        schedule = fastf1.get_event_schedule(year, include_testing=False)\n",
    "        races_before = schedule[schedule['RoundNumber'] < target_round]\n",
    "\n",
    "        if races_before.empty:\n",
    "            return {}, 0, {}, 0\n",
    "\n",
    "        for index, race in races_before.iterrows():\n",
    "            try:\n",
    "                session = fastf1.get_session(year, race['EventName'], 'R')\n",
    "                session.load(laps=False, telemetry=False, weather=False, messages=False)\n",
    "                results = session.results\n",
    "                if results is not None and not results.empty:\n",
    "                    for _, driver_result in results.iterrows():\n",
    "                        driver = driver_result['Abbreviation']\n",
    "                        team = driver_result['TeamName']\n",
    "                        points = driver_result['Points']\n",
    "                        # Update driver standings\n",
    "                        driver_standings[driver] = driver_standings.get(driver, 0) + points\n",
    "                        # Update constructor standings\n",
    "                        constructor_standings[team] = constructor_standings.get(team, 0) + points\n",
    "            except Exception as e:\n",
    "                # Silently continue if a past race fails to load\n",
    "                continue\n",
    "\n",
    "        if driver_standings:\n",
    "            max_driver_points = max(driver_standings.values()) if driver_standings else 0\n",
    "        if constructor_standings:\n",
    "             max_constructor_points = max(constructor_standings.values()) if constructor_standings else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating standings for {year} before round {target_round}: {e}\")\n",
    "        return {}, 0, {}, 0\n",
    "\n",
    "    return driver_standings, max_driver_points, constructor_standings, max_constructor_points\n",
    "\n",
    "def get_race_data(year, event_identifier):\n",
    "    try:\n",
    "        session_r = fastf1.get_session(year, event_identifier, 'R')\n",
    "        session_r.load(laps=False, telemetry=False, weather=False, messages=False)\n",
    "        results_r = session_r.results\n",
    "\n",
    "        if results_r is None or results_r.empty:\n",
    "            print(f\"No race results found for {year} {event_identifier}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        results_r['FinishingPosition'] = pd.to_numeric(results_r['Position'], errors='coerce')\n",
    "        results_r['GridPosition'] = pd.to_numeric(results_r['GridPosition'], errors='coerce')  # Add GridPosition\n",
    "        results_r.dropna(subset=['FinishingPosition', 'GridPosition'], inplace=True)  # Ensure both positions are valid\n",
    "        results_r[['FinishingPosition', 'GridPosition']] = results_r[['FinishingPosition', 'GridPosition']].astype(int)\n",
    "\n",
    "        session_q = fastf1.get_session(year, event_identifier, 'Q')\n",
    "        session_q.load(laps=False, telemetry=False, weather=False, messages=False)\n",
    "        results_q = session_q.results\n",
    "\n",
    "        if results_q is None or results_q.empty:\n",
    "            print(f\"No qualifying results found for {year} {event_identifier}. Proceeding without Quali times.\")\n",
    "            qualifying_times = pd.DataFrame({'Abbreviation': results_r['Abbreviation'], 'QualifyingTime (s)': np.nan})\n",
    "        else:\n",
    "            results_q['QualifyingTime'] = results_q[['Q1', 'Q2', 'Q3']].min(axis=1)\n",
    "            results_q['QualifyingTime (s)'] = results_q['QualifyingTime'].dt.total_seconds()\n",
    "            qualifying_times = results_q[['Abbreviation', 'QualifyingTime (s)']].copy()\n",
    "        \n",
    "        race_data = pd.merge(results_r[['Abbreviation', 'TeamName', 'FinishingPosition', 'GridPosition']], \n",
    "                             qualifying_times, on='Abbreviation', how='left')\n",
    "        \n",
    "        race_data['Year'] = year\n",
    "        race_data['RoundNumber'] = session_r.event['RoundNumber']\n",
    "        race_data['EventName'] = session_r.event['EventName']\n",
    "        \n",
    "        # Add driver characteristics\n",
    "        for idx, row in race_data.iterrows():\n",
    "            driver_code = row['Abbreviation']\n",
    "            driver_features = get_driver_features(driver_code)\n",
    "            for feature_name, feature_value in driver_features.items():\n",
    "                race_data.at[idx, feature_name] = feature_value\n",
    "        \n",
    "        # Calculate Points Index *before* this race using the updated function\n",
    "        driver_standings, max_driver_points, constructor_standings, max_constructor_points = get_standings_before_round(year, race_data['RoundNumber'].iloc[0])\n",
    "        \n",
    "        # Calculate Driver Points Index\n",
    "        if max_driver_points > 0:\n",
    "            race_data['PointsIndex'] = race_data['Abbreviation'].apply(lambda x: driver_standings.get(x, 0) / max_driver_points)\n",
    "        else:\n",
    "            race_data['PointsIndex'] = 0.0\n",
    "            \n",
    "        # Calculate Constructor Points Index\n",
    "        if max_constructor_points > 0:\n",
    "            race_data['ConstructorPointsIndex'] = race_data['TeamName'].apply(lambda x: constructor_standings.get(x, 0) / max_constructor_points)\n",
    "        else:\n",
    "             race_data['ConstructorPointsIndex'] = 0.0\n",
    "             \n",
    "        race_data.rename(columns={'Abbreviation': 'Driver', 'TeamName': 'Team'}, inplace=True)\n",
    "        \n",
    "        mean_quali_time = race_data['QualifyingTime (s)'].mean()\n",
    "        race_data['QualifyingTime (s)'] = race_data['QualifyingTime (s)'].fillna(mean_quali_time)\n",
    "        \n",
    "        # Ensure no NaNs remain in core features\n",
    "        core_features = ['Driver', 'Team', 'FinishingPosition', 'QualifyingTime (s)', \n",
    "                         'PointsIndex', 'ConstructorPointsIndex', 'GridPosition']\n",
    "        driver_feature_keys = list(get_driver_features('VER').keys())  # Get a sample of driver feature keys\n",
    "        core_features.extend(driver_feature_keys)\n",
    "        race_data.dropna(subset=core_features, inplace=True)\n",
    "        \n",
    "        # Include driver features in the returned columns\n",
    "        return_columns = ['Year', 'EventName', 'Driver', 'Team', 'QualifyingTime (s)', \n",
    "                          'PointsIndex', 'ConstructorPointsIndex', 'GridPosition', 'FinishingPosition']\n",
    "        return_columns.extend(driver_feature_keys)\n",
    "        \n",
    "        return race_data[return_columns]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {year} {event_identifier}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Configuration ---\n",
    "TARGET_YEAR = 2025\n",
    "TARGET_GP_NAME = 'Saudi' \n",
    "HISTORICAL_YEARS = range(TARGET_YEAR - 1, 2017, -1) \n",
    "\n",
    "\n",
    "all_race_data = []\n",
    "\n",
    "try:\n",
    "    schedule_target_year = fastf1.get_event_schedule(TARGET_YEAR, include_testing=False)\n",
    "    target_event = schedule_target_year[schedule_target_year['EventName'].str.contains(TARGET_GP_NAME, case=False, na=False)]\n",
    "    \n",
    "    if target_event.empty:\n",
    "        raise ValueError(f\"Target GP '{TARGET_GP_NAME}' not found in {TARGET_YEAR} schedule.\")\n",
    "        \n",
    "    target_round = target_event['RoundNumber'].iloc[0]\n",
    "    target_venue = target_event['Location'].iloc[0]\n",
    "    target_event_name = target_event['EventName'].iloc[0] # Official name for consistency\n",
    "    print(f\"Target Event: {TARGET_YEAR} {target_event_name} (Round {target_round}, Venue: {target_venue})\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error finding target GP info: {e}\")\n",
    "\n",
    "# 2. Get Data from Current Season (up to target GP)\n",
    "print(f\"\\n--- Fetching {TARGET_YEAR} data before Round {target_round} ---\")\n",
    "schedule_current_year_filtered = schedule_target_year[schedule_target_year['RoundNumber'] < target_round]\n",
    "for index, race in schedule_current_year_filtered.iterrows():\n",
    "    print(f\"Processing {TARGET_YEAR} {race['EventName']}...\")\n",
    "    data = get_race_data(TARGET_YEAR, race['EventName'])\n",
    "    if data is not None:\n",
    "        all_race_data.append(data)\n",
    "\n",
    "# 3. Get Historical Data\n",
    "print(f\"\\n--- Fetching Historical Data ---\")\n",
    "for year in HISTORICAL_YEARS:\n",
    "    try:\n",
    "        schedule_hist = fastf1.get_event_schedule(year, include_testing=False)\n",
    "        print(f\"Processing all races for {year}...\")\n",
    "        # Iterate through all races in the historical year's schedule\n",
    "        for index, race_hist in schedule_hist.iterrows():\n",
    "            event_name_hist = race_hist['EventName']\n",
    "            print(f\"  Processing {year} {event_name_hist}...\")\n",
    "            data = get_race_data(year, event_name_hist)\n",
    "            if data is not None:\n",
    "                all_race_data.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting schedule or processing races for {year}: {e}\")\n",
    "\n",
    "# 4. Combine Data\n",
    "if not all_race_data:\n",
    "    raise ValueError(\"No data could be collected. Check FastF1 setup and GP/Year validity.\")\n",
    "\n",
    "combined_data = pd.concat(all_race_data, ignore_index=True)\n",
    "\n",
    "# Drop the RoundNumber column\n",
    "if 'RoundNumber' in combined_data.columns:\n",
    "    combined_data.drop(columns=['RoundNumber'], inplace=True)\n",
    "    print(\"Dropped 'RoundNumber' column.\")\n",
    "else:\n",
    "    print(\"'RoundNumber' column not found in combined_data.\")\n",
    "\n",
    "print(f\"\\n--- Combined Data Shape after dropping RoundNumber: {combined_data.shape} ---\")\n",
    "print(combined_data.head())\n",
    "print(combined_data.info())\n",
    "print(combined_data.describe())\n",
    "\n",
    "# Export the combined dataset to CSV\n",
    "try:\n",
    "    csv_filename = 'combined_f1_data.csv'\n",
    "    combined_data.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\n--- Full Combined Dataset exported to {csv_filename} ---\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError exporting data to CSV: {e}\")\n",
    "\n",
    "# 5. Prepare Data for Target Race Prediction\n",
    "print(f\"\\n--- Preparing data for {TARGET_YEAR} {target_event_name} prediction ---\")\n",
    "try:\n",
    "    # Get Qualifying data for the actual target race\n",
    "    target_q_session = fastf1.get_session(TARGET_YEAR, target_event_name, 'Q')\n",
    "    target_q_session.load(laps=False, telemetry=False, weather=False, messages=False)\n",
    "    target_q_results = target_q_session.results\n",
    "    \n",
    "    if target_q_results is None or target_q_results.empty:\n",
    "         raise ValueError(f\"Could not load Qualifying results for target event: {TARGET_YEAR} {target_event_name}\")\n",
    "         \n",
    "    target_q_results['QualifyingTime'] = target_q_results[['Q1', 'Q2', 'Q3']].min(axis=1)\n",
    "    target_q_results['QualifyingTime (s)'] = target_q_results['QualifyingTime'].dt.total_seconds()\n",
    "    \n",
    "    # Get points index before the target race using the updated function\n",
    "    target_driver_standings, target_max_driver_points, target_constructor_standings, target_max_constructor_points = get_standings_before_round(TARGET_YEAR, target_round)\n",
    "    \n",
    "    # Create DataFrame for prediction\n",
    "    predict_df = target_q_results[['Abbreviation', 'TeamName', 'QualifyingTime (s)']].copy()\n",
    "    predict_df.rename(columns={'Abbreviation': 'Driver', 'TeamName': 'Team'}, inplace=True)\n",
    "    \n",
    "    # Calculate Driver Points Index for prediction set\n",
    "    if target_max_driver_points > 0:\n",
    "         predict_df['PointsIndex'] = predict_df['Driver'].apply(lambda x: target_driver_standings.get(x, 0) / target_max_driver_points)\n",
    "    else:\n",
    "         predict_df['PointsIndex'] = 0.0\n",
    "         \n",
    "    # Calculate Constructor Points Index for prediction set\n",
    "    if target_max_constructor_points > 0:\n",
    "         predict_df['ConstructorPointsIndex'] = predict_df['Team'].apply(lambda x: target_constructor_standings.get(x, 0) / target_max_constructor_points)\n",
    "    else:\n",
    "         predict_df['ConstructorPointsIndex'] = 0.0\n",
    "         \n",
    "    # Handle potential missing Quali times in prediction set (e.g., pit lane start)\n",
    "   \n",
    "    mean_quali_train = combined_data['QualifyingTime (s)'].mean()\n",
    "    predict_df['QualifyingTime (s)'] = predict_df['QualifyingTime (s)'].fillna(mean_quali_train)\n",
    "    \n",
    "    # Sort the qualifying results by time to determine grid positions\n",
    "    predict_df = predict_df.sort_values('QualifyingTime (s)')\n",
    "    predict_df['GridPosition'] = range(1, len(predict_df) + 1)\n",
    "    \n",
    "    # Add driver features to the prediction dataframe for the target race\n",
    "    for idx, row in predict_df.iterrows():\n",
    "        driver_code = row['Driver']\n",
    "        driver_features = get_driver_features(driver_code)\n",
    "        for feature_name, feature_value in driver_features.items():\n",
    "            predict_df.at[idx, feature_name] = feature_value\n",
    "    \n",
    "    # Ensure no NaNs in prediction features\n",
    "    predict_df.dropna(subset=['QualifyingTime (s)', 'PointsIndex', 'ConstructorPointsIndex'], inplace=True)\n",
    "    \n",
    "    # Keep track of drivers for final output\n",
    "    predict_drivers = predict_df['Driver'].tolist()\n",
    "    \n",
    "    print(f\"Prediction input data shape: {predict_df.shape}\")\n",
    "    print(predict_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error preparing prediction data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e56213c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    GridPosition  QualifyingTime (s)  PointsIndex  ConstructorPointsIndex  \\\n",
      "1              1              87.294     0.828947                0.454545   \n",
      "81             2              87.304     0.881579                1.000000   \n",
      "63             3              87.407     0.763158                0.601399   \n",
      "4              4              87.481     1.000000                1.000000   \n",
      "16             5              87.670     0.368421                0.314685   \n",
      "\n",
      "    QualifyingPace  RaceCraft  Consistency  Aggression  TireManagement  \n",
      "1             0.95       0.95         0.90        0.90            0.85  \n",
      "81            0.88       0.84         0.85        0.78            0.85  \n",
      "63            0.90       0.85         0.85        0.83            0.82  \n",
      "4             0.92       0.88         0.88        0.75            0.87  \n",
      "16            0.95       0.85         0.80        0.85            0.80  \n",
      "\\Predicted Finishing Order for 2025 Saudi Arabian Grand Prix \n",
      "\n",
      "Driver  PredictedFinishingPosition\n",
      "   PIA                           1\n",
      "   NOR                           2\n",
      "   ANT                           3\n",
      "   RUS                           4\n",
      "   LEC                           5\n",
      "   HAM                           6\n",
      "   VER                           7\n",
      "   TSU                           8\n",
      "   SAI                           9\n",
      "   GAS                          10\n",
      "   ALB                          11\n",
      "   LAW                          12\n",
      "   ALO                          13\n",
      "   BEA                          14\n",
      "   OCO                          15\n",
      "   HAD                          16\n",
      "   HUL                          17\n",
      "   STR                          18\n",
      "   DOO                          19\n",
      "   BOR                          20\n",
      "\n",
      "--- Evaluating Model Performance ---\n",
      "Mean Absolute Error on test set is 3.30 positions\n",
      "\n",
      "Feature Importance\n",
      "               Feature  Importance\n",
      "          GridPosition    0.447762\n",
      "ConstructorPointsIndex    0.189449\n",
      "           PointsIndex    0.162565\n",
      "    QualifyingTime (s)    0.156560\n",
      "             RaceCraft    0.011034\n",
      "        TireManagement    0.010176\n",
      "           Consistency    0.008344\n",
      "        QualifyingPace    0.008133\n",
      "            Aggression    0.005977\n"
     ]
    }
   ],
   "source": [
    "features = ['GridPosition', 'QualifyingTime (s)', 'PointsIndex', 'ConstructorPointsIndex','QualifyingPace', 'RaceCraft', 'Consistency', 'Aggression', 'TireManagement']\n",
    "target = 'FinishingPosition'\n",
    "\n",
    "X = combined_data[features].copy()\n",
    "y = combined_data[target].copy()\n",
    "\n",
    "predict_X = predict_df[features].copy()\n",
    "\n",
    "\n",
    "print(predict_X.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.2, \n",
    "    max_depth=3, \n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predicted_scores = model.predict(predict_X)\n",
    "\n",
    "#Create Ranked Results\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Driver': predict_drivers, \n",
    "    'PredictedScore': predicted_scores\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values(by='PredictedScore')\n",
    "# Assign ranks \n",
    "results_df['PredictedFinishingPosition'] = np.arange(1, len(results_df) + 1)\n",
    "\n",
    "\n",
    "print(f\"\\Predicted Finishing Order for {TARGET_YEAR} {target_event_name} \\n\")\n",
    "print(results_df[['Driver', 'PredictedFinishingPosition']].to_string(index=False))\n",
    "\n",
    "\n",
    "print(\"\\n--- Evaluating Model Performance ---\")\n",
    "y_pred_test = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "print(f\"Mean Absolute Error on test set is {mae:.2f} positions\")\n",
    "\n",
    "print(\"\\nFeature Importance\")\n",
    "try:\n",
    "    feature_names = X.columns\n",
    "    importances = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    print(importance_df.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate or display feature importances: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebca33ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Preparing Data for Random Forest Regressor ---\n",
      "\\n--- Training Random Forest Regressor ---\n",
      "\\n--- Predicting with Random Forest Regressor for 2025 Saudi Arabian Grand Prix ---\n",
      "\\nPredicted Finishing Order (Random Forest) for 2025 Saudi Arabian Grand Prix\\n\n",
      "Driver  PredictedFinishingPosition\n",
      "   PIA                           1\n",
      "   NOR                           2\n",
      "   RUS                           3\n",
      "   VER                           4\n",
      "   LEC                           5\n",
      "   ANT                           6\n",
      "   HAM                           7\n",
      "   GAS                           8\n",
      "   TSU                           9\n",
      "   SAI                          10\n",
      "   ALO                          11\n",
      "   ALB                          12\n",
      "   HAD                          13\n",
      "   BEA                          14\n",
      "   LAW                          15\n",
      "   OCO                          16\n",
      "   DOO                          17\n",
      "   BOR                          18\n",
      "   STR                          19\n",
      "   HUL                          20\n",
      "\\n--- Evaluating Random Forest Model Performance ---\n",
      "Mean Absolute Error on test set is 3.45 positions\n",
      "\\nFeature Importance (Random Forest)\n",
      "               Feature  Importance\n",
      "          GridPosition    0.327158\n",
      "           PointsIndex    0.204172\n",
      "ConstructorPointsIndex    0.196284\n",
      "    QualifyingTime (s)    0.180855\n",
      "            Aggression    0.021664\n",
      "        QualifyingPace    0.018712\n",
      "           Consistency    0.018246\n",
      "             RaceCraft    0.017812\n",
      "        TireManagement    0.015097\n"
     ]
    }
   ],
   "source": [
    "# Cell for Random Forest Regressor Implementation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming train_test_split and mean_absolute_error are already imported\n",
    "# Assuming combined_data, predict_df, predict_drivers, X_train, X_test, y_train, y_test are available\n",
    "\n",
    "print(\"\\\\n--- Preparing Data for Random Forest Regressor ---\")\n",
    "\n",
    "# Use the same features and target as before\n",
    "features_rf = ['GridPosition', 'QualifyingTime (s)', 'PointsIndex', 'ConstructorPointsIndex',\n",
    "               'QualifyingPace', 'RaceCraft', 'Consistency', 'Aggression', 'TireManagement']\n",
    "target_rf = 'FinishingPosition'\n",
    "\n",
    "# use training and test set from gradient boosting \n",
    "\n",
    "# --- Model Training ---\n",
    "print(\"\\\\n--- Training Random Forest Regressor ---\")\n",
    "\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    n_estimators=100,      \n",
    "    max_depth=None,         \n",
    "    min_samples_split=2,    \n",
    "    min_samples_leaf=1,    \n",
    "    random_state=42,\n",
    "    n_jobs=-1              \n",
    ")\n",
    "\n",
    "rf_regressor.fit(X_train, y_train) \n",
    "\n",
    "# --- Prediction ---\n",
    "print(f\"\\\\n--- Predicting with Random Forest Regressor for {TARGET_YEAR} {target_event_name} ---\")\n",
    "\n",
    "\n",
    "predict_X_rf = predict_df[features_rf].copy() \n",
    "\n",
    "predicted_rf_scores = rf_regressor.predict(predict_X_rf)\n",
    "\n",
    "# Create Ranked Results\n",
    "results_rf_df = pd.DataFrame({\n",
    "    'Driver': predict_drivers, \n",
    "    'PredictedRFScore': predicted_rf_scores \n",
    "})\n",
    "\n",
    "results_rf_df = results_rf_df.sort_values(by='PredictedRFScore')\n",
    "results_rf_df['PredictedFinishingPosition'] = np.arange(1, len(results_rf_df) + 1)\n",
    "\n",
    "print(f\"\\\\nPredicted Finishing Order (Random Forest) for {TARGET_YEAR} {target_event_name}\\\\n\")\n",
    "print(results_rf_df[['Driver', 'PredictedFinishingPosition']].to_string(index=False))\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\\\n--- Evaluating Random Forest Model Performance ---\")\n",
    "y_pred_test_rf = rf_regressor.predict(X_test)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_test_rf) \n",
    "print(f\"Mean Absolute Error on test set is {mae_rf:.2f} positions\")\n",
    "\n",
    "# --- Feature Importance ---\n",
    "print(\"\\\\nFeature Importance (Random Forest)\")\n",
    "try:\n",
    "    feature_names_rf = X_train.columns\n",
    "    importances_rf = rf_regressor.feature_importances_\n",
    "    importance_rf_df = pd.DataFrame({'Feature': feature_names_rf, 'Importance': importances_rf})\n",
    "    importance_rf_df = importance_rf_df.sort_values(by='Importance', ascending=False)\n",
    "    print(importance_rf_df.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate or display feature importances: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6cbc8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Preparing Data for LambdaMART (LightGBM Ranker) ---\n",
      "\\n--- Performing Train-Test Split for LambdaMART ---\n",
      "\\n--- Training LambdaMART (LightGBM Ranker) ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 823\n",
      "[LightGBM] [Info] Number of data points in the train set: 1759, number of used features: 5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's ndcg@1: 0.606997\tvalid_0's ndcg@3: 0.721274\tvalid_0's ndcg@5: 0.771753\tvalid_0's ndcg@10: 0.817031\tvalid_0's ndcg@20: 0.82769\n",
      "\\n--- Predicting with LambdaMART for 2025 Saudi Arabian Grand Prix ---\n",
      "\\nPredicted Finishing Order (LambdaMART) for 2025 Saudi Arabian Grand Prix\\n\n",
      "Driver  PredictedFinishingPosition\n",
      "   PIA                           1\n",
      "   VER                           2\n",
      "   NOR                           3\n",
      "   RUS                           4\n",
      "   LEC                           5\n",
      "   ANT                           6\n",
      "   HAM                           7\n",
      "   TSU                           8\n",
      "   SAI                           9\n",
      "   GAS                          10\n",
      "   ALB                          11\n",
      "   BEA                          12\n",
      "   OCO                          13\n",
      "   LAW                          14\n",
      "   HAD                          15\n",
      "   ALO                          16\n",
      "   STR                          17\n",
      "   DOO                          18\n",
      "   HUL                          19\n",
      "   BOR                          20\n",
      "\\n--- Evaluating LambdaMART Model Performance on Test Set ---\n",
      "Mean NDCG@20 (Ranking Accuracy Percentage) on test set: 94.47%\n",
      "\\nFeature Importance (LambdaMART)\n",
      "               Feature  Importance\n",
      "          GridPosition           9\n",
      "ConstructorPointsIndex           4\n",
      "           PointsIndex           3\n",
      "    QualifyingTime (s)           2\n",
      "                Driver           1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\AI Project\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "e:\\Projects\\AI Project\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n",
      "e:\\Projects\\AI Project\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:861: UserWarning: Found 'ndcg_eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    }
   ],
   "source": [
    "# Cell for LambdaMART (LightGBM Ranker) Implementation\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "print(\"\\\\n--- Preparing Data for LambdaMART (LightGBM Ranker) ---\")\n",
    "\n",
    "\n",
    "features_ltr = ['Driver', 'GridPosition', 'QualifyingTime (s)', 'PointsIndex', 'ConstructorPointsIndex']\n",
    "target_ltr = 'RelevanceScore'\n",
    "\n",
    "# --- Data Preparation for LTR ---\n",
    "combined_data_ltr = combined_data.copy()\n",
    "\n",
    "\n",
    "\n",
    "combined_data_ltr['Driver'] = combined_data_ltr['Driver'].astype('category')\n",
    "\n",
    "combined_data_ltr[target_ltr] = 21 - combined_data_ltr['FinishingPosition']\n",
    "combined_data_ltr['QueryID'] = combined_data_ltr.groupby(['Year', 'EventName']).ngroup()\n",
    "\n",
    "X_ltr_full = combined_data_ltr[features_ltr]\n",
    "y_ltr_full = combined_data_ltr[target_ltr]\n",
    "groups_ltr_full = combined_data_ltr['QueryID']\n",
    "\n",
    "# --- Train-Test Split for LTR ---\n",
    "print(\"\\\\n--- Performing Train-Test Split for LambdaMART ---\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X_ltr_full, y_ltr_full, groups_ltr_full))\n",
    "\n",
    "X_train_ltr, X_test_ltr = X_ltr_full.iloc[train_idx], X_ltr_full.iloc[test_idx]\n",
    "y_train_ltr, y_test_ltr = y_ltr_full.iloc[train_idx], y_ltr_full.iloc[test_idx]\n",
    "\n",
    "groups_train_ltr_counts = combined_data_ltr.iloc[train_idx].groupby('QueryID').size().tolist()\n",
    "query_ids_test_ltr = combined_data_ltr.iloc[test_idx]['QueryID']\n",
    "groups_test_ltr_counts = combined_data_ltr.iloc[test_idx].groupby('QueryID').size().tolist()\n",
    "\n",
    "# --- Model Training ---\n",
    "print(\"\\\\n--- Training LambdaMART (LightGBM Ranker) ---\")\n",
    "\n",
    "lgbm_ranker = lgb.LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    ndcg_eval_at=[1, 3, 5, 10, 20],\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    # No need for categorical_feature in fit if dtypes are 'category'\n",
    ")\n",
    "\n",
    "if not X_train_ltr.empty and not X_test_ltr.empty and groups_train_ltr_counts and groups_test_ltr_counts:\n",
    "    lgbm_ranker.fit(X_train_ltr, y_train_ltr, group=groups_train_ltr_counts,\n",
    "                    eval_set=[(X_test_ltr, y_test_ltr)],\n",
    "                    eval_group=[groups_test_ltr_counts], # Corrected to pass the list of group sizes\n",
    "                    eval_metric='ndcg',\n",
    "                    eval_at=[1, 3, 5, 10, 20],\n",
    "                    callbacks=[lgb.early_stopping(10, verbose=True)])\n",
    "else:\n",
    "    print(\"Training or Test set for LTR is empty or group counts are missing. Skipping LTR model training and evaluation.\")\n",
    "\n",
    "# --- Prediction on Target Event ---\n",
    "if hasattr(lgbm_ranker, 'booster_') and lgbm_ranker.booster_: # Check if model was trained\n",
    "    print(f\"\\\\n--- Predicting with LambdaMART for {TARGET_YEAR} {target_event_name} ---\")\n",
    "\n",
    "    # Ensure 'Driver' in predict_df is also category and has same categories as training data\n",
    "    predict_df_ltr = predict_df.copy()\n",
    "    predict_df_ltr['Driver'] = pd.Categorical(predict_df_ltr['Driver'], categories=X_train_ltr['Driver'].cat.categories)\n",
    "    predict_X_ltr = predict_df_ltr[features_ltr]\n",
    "\n",
    "   \n",
    "\n",
    "    predicted_ltr_scores_target_event = lgbm_ranker.predict(predict_X_ltr)\n",
    "\n",
    "    results_ltr_df_target_event = pd.DataFrame({\n",
    "        'Driver': predict_drivers, # predict_drivers should be available from your previous cells\n",
    "        'PredictedLTRScore': predicted_ltr_scores_target_event\n",
    "    })\n",
    "    results_ltr_df_target_event = results_ltr_df_target_event.sort_values(by='PredictedLTRScore', ascending=False)\n",
    "    results_ltr_df_target_event['PredictedFinishingPosition'] = np.arange(1, len(results_ltr_df_target_event) + 1)\n",
    "\n",
    "    print(f\"\\\\nPredicted Finishing Order (LambdaMART) for {TARGET_YEAR} {target_event_name}\\\\n\")\n",
    "    print(results_ltr_df_target_event[['Driver', 'PredictedFinishingPosition']].to_string(index=False))\n",
    "\n",
    "    # --- Evaluation on Test Set ---\n",
    "    print(\"\\\\n--- Evaluating LambdaMART Model Performance on Test Set ---\")\n",
    "    if not X_test_ltr.empty:\n",
    "        test_preds_ltr = lgbm_ranker.predict(X_test_ltr)\n",
    "        ndcg_scores_per_query = []\n",
    "        k_val_overall = 20\n",
    "        for query_id in sorted(query_ids_test_ltr.unique()):\n",
    "            query_mask = (query_ids_test_ltr == query_id)\n",
    "            true_relevance_for_query = y_test_ltr[query_mask].values.reshape(1, -1)\n",
    "            predicted_scores_for_query = test_preds_ltr[query_mask].reshape(1, -1)\n",
    "\n",
    "            if true_relevance_for_query.shape[1] > 1:\n",
    "                k_val = min(k_val_overall, true_relevance_for_query.shape[1])\n",
    "                if k_val > 0:\n",
    "                    ndcg_val = ndcg_score(true_relevance_for_query, predicted_scores_for_query, k=k_val)\n",
    "                    ndcg_scores_per_query.append(ndcg_val)\n",
    "\n",
    "        if ndcg_scores_per_query:\n",
    "            mean_ndcg = np.mean(ndcg_scores_per_query)\n",
    "            print(f\"Mean NDCG@{k_val_overall} (Ranking Accuracy Percentage) on test set: {mean_ndcg*100:.2f}%\") # Using the modified print\n",
    "        else:\n",
    "            print(\"Could not calculate Mean NDCG. Test set might be too small or group distribution issues.\")\n",
    "    else:\n",
    "        print(\"Test set for LTR is empty. Skipping NDCG evaluation.\")\n",
    "\n",
    "    print(\"\\\\nFeature Importance (LambdaMART)\")\n",
    "    try:\n",
    "        feature_names_ltr = X_train_ltr.columns\n",
    "        importances_ltr = lgbm_ranker.feature_importances_\n",
    "        importance_ltr_df = pd.DataFrame({'Feature': feature_names_ltr, 'Importance': importances_ltr})\n",
    "        importance_ltr_df = importance_ltr_df.sort_values(by='Importance', ascending=False)\n",
    "        print(importance_ltr_df.to_string(index=False))\n",
    "    except Exception as e:\n",
    "            print(f\"Could not calculate or display feature importances: {e}\")\n",
    "else:\n",
    "    print(\"LGBMRanker model was not trained (likely due to empty train/test sets or other issues). Skipping target prediction and evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
